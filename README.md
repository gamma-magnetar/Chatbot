PPT-PDF RAG Chatbot (Text + Vision)
Overview

This project implements a Retrieval-Augmented Generation (RAG) chatbot that can answer questions from PPT-based PDFs, including image-heavy slides and charts.

The system supports:

âœ… Text-based PDFs (reports, documents)

âœ… Image-heavy PPT PDFs (slides, charts, diagrams)

âœ… Vision-based understanding of charts and visuals

âœ… Grounded answers (no hallucinations)

It combines PDF parsing, OCR, vision-LLMs, vector search (FAISS), and LLMs into a single Streamlit application.

ğŸ§  High-Level Architecture
4
Pipeline Flow
PDF Upload
   â†“
Text Extraction (pdfplumber)
   â†“
Image OCR (Tesseract)
   â†“
Vision-based Slide Understanding (GPT-4o / Vision LLM)
   â†“
Chunking
   â†“
Embeddings (Sentence Transformers)
   â†“
FAISS Vector Store
   â†“
User Query
   â†“
Context Retrieval
   â†“
LLM Answer (Grounded Response)

âœ¨ Key Features

Multimodal RAG

Handles both text and image-only PPT PDFs

Chart & Slide Understanding

Vision LLM explains charts, trends, comparisons

Hallucination-Safe

Answers only from retrieved document context

Modular & Extensible

OCR, vision, embeddings, and retrieval are cleanly separated

Interactive UI

Streamlit-based web interface

ğŸ§  Models Used
1ï¸âƒ£ Embedding Model

Model: sentence-transformers/all-MiniLM-L6-v2

Purpose: Convert document chunks into vector embeddings

Why: Fast, lightweight, high-quality semantic search

2ï¸âƒ£ Language Model (Text)

Model: OpenAI GPT-4o / GPT-4o-mini

Purpose: Generate final answers from retrieved context

Behavior: Strictly grounded in retrieved chunks

3ï¸âƒ£ Vision Model (Charts & Images)

Model: GPT-4o (Vision)

Purpose:

Understand charts visually

Explain trends, comparisons, and insights

Convert visual information into text for RAG

Why Vision is required:

OCR alone cannot understand chart relationships or trends

ğŸ“ Project Structure
ppt-pdf-rag-chatbot/
â”‚
â”œâ”€â”€ app.py                # Streamlit UI & session management
â”œâ”€â”€ ingest.py             # PDF ingestion & vector index creation
â”œâ”€â”€ rag.py                # Retrieval + LLM answering logic
â”œâ”€â”€ ocr_utils.py          # Image OCR utilities
â”œâ”€â”€ vision_utils.py       # Vision-based slide & chart understanding
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â”‚
â””â”€â”€ data/                 # (Ignored) Runtime data, FAISS index, uploads

âš™ï¸ How It Works (Detailed)
Step 1: PDF Ingestion

Extracts selectable text using pdfplumber

Extracts embedded images and applies OCR

Detects image-heavy PDFs

Step 2: Vision-Based Understanding (Image-Heavy PDFs)

Converts each slide/page into an image

Sends image to a vision-capable LLM

Generates a textual explanation of charts and visuals

Step 3: Chunking & Embeddings

All extracted text (text + OCR + vision summaries) is chunked

Each chunk is embedded using Sentence Transformers

Step 4: Vector Storage

Embeddings stored in FAISS

Enables fast semantic retrieval

Step 5: Question Answering (RAG)

User query is embedded

Top-k relevant chunks retrieved

LLM answers using only retrieved context

ğŸ–¥ï¸ Running the App Locally
1ï¸âƒ£ Clone the repository
git clone https://github.com/<your-username>/<repo-name>.git
cd <repo-name>

2ï¸âƒ£ Create and activate virtual environment
python -m venv venv
# Windows
venv\Scripts\activate
# Mac/Linux
source venv/bin/activate

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Set API Key
# Windows (PowerShell)
setx OPENAI_API_KEY "your_api_key_here"

# Mac/Linux
export OPENAI_API_KEY="your_api_key_here"


Restart the terminal after setting the key.

5ï¸âƒ£ Run Streamlit
streamlit run app.py


Open in browser:

http://localhost:8501

ğŸš¦ Usage Instructions

Upload a PDF (including PPT-based PDFs)

Wait for ingestion to complete

Ask questions like:

â€œWhat trend does the revenue chart show?â€

â€œSummarize the key insights from this presentationâ€

â€œWhat conclusion is drawn in the final slides?â€

If the document contains no usable content, the app safely disables querying.

âš ï¸ Notes on API Usage

Vision models require sufficient API quota

If quota is exhausted:

Text-based PDFs still work

Vision ingestion may fail gracefully

Vision extraction is triggered only for image-heavy PDFs to reduce cost

ğŸ”® Future Improvements

Slide-level citations in answers

Caching vision summaries to reduce cost

Toggle between OCR-only and Vision-mode

Hybrid search (BM25 + vector)

Multi-PDF knowledge base

ğŸ“Œ Summary

This project demonstrates a production-style multimodal RAG system capable of understanding text, images, and charts inside PPT-based PDFs, combining:

Document intelligence

Vision-LLMs

Vector databases

Streamlit deployment
